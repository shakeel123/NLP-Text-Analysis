{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220ed903",
   "metadata": {},
   "source": [
    "### Text Analytics\n",
    "As we have seen cases of different types of data and different ways of processing them to obtain important information out of them, in this section we will talk about one of the most sought upon data in today’s time. Customer Information is the backbone for most of the renowned companies in 21st century. Let’s take example of Google. How do you suppose Google is providing so many services for free? What profit does it make by giving you free services? Well it’s your information that google sells to different companies by analyzing your searches and makes profit through it. The moment you search for a product on google, you will start seeing the product recommendation on every website. This is the most basic usage of text analytics, product recommendation to customers by analyzing their searches. Similarly, many companies analyze the positive or negative reviews given by customers and try to predict the customer behavior. There is a wealth of such unstructured data present such as emails, google searches, online surveys, twitter, online reviews etc. which can be processed using text analysis. Many key information about people, customers can be derived by processing the unstructured text and analyzing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94bd002",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP)\n",
    "\n",
    "\n",
    "Natural language processing (NLP) is the discipline of building machines that can manipulate human language — or data that resembles human language — in the way that it is written, spoken, and organized. It evolved from computational linguistics, which uses computer science to understand the principles of language, but rather than developing theoretical frameworks, NLP is an engineering discipline that seeks to build technology to accomplish useful tasks. NLP can be divided into two overlapping subfields: natural language understanding (NLU), which focuses on semantic analysis or determining the intended meaning of text, and natural language generation (NLG), which focuses on text generation by a machine. NLP is separate from — but often used in conjunction with — speech recognition, which seeks to parse spoken language into words, turning sound into text and vice versa. \n",
    "\n",
    "Siri and Alexa are one such example of uses of NLP.\n",
    "\n",
    "\n",
    "We will use NLP for text analytics.\n",
    "\n",
    "\n",
    "There many libraries available for NLP in python. we will focus on the two most important one's :\n",
    "\n",
    "* Natural Languange Tool Kit (NLTK)\n",
    "* Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2ec193",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is a process of breaking down a given paragraph of text into a list of sentence or words. When paragraph is broken down into list of sentences, it is called sentence tokenization.\n",
    "Similarly, if the sentences are further broken down into list of words, it is known as Word tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d9a6cf",
   "metadata": {},
   "source": [
    "### Example Text\n",
    "Below is a given paragraph, let's see how tokenization works on it:\n",
    "\n",
    "Denmark is a Scandinavian country comprising the Jutland Peninsula and numerous islands. It's linked to nearby Sweden via the Öresund bridge. Copenhagen, its capital, is home to royal palaces and colorful Nyhavn harbor, plus the Tivoli amusement park and the iconic “Little Mermaid” statue. Odense is writer Hans Christian Andersen’s hometown, with a medieval core of cobbled streets and half-timbered houses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b607f",
   "metadata": {},
   "source": [
    "Let's understand about some important terminologies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3e4a653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Denmark is a Scandinavian country comprising the Jutland Peninsula and numerous islands.',\n",
       " \"It's linked to nearby Sweden via the Öresund bridge.\",\n",
       " 'Copenhagen, its capital, is home to royal palaces and colorful Nyhavn harbor, plus the Tivoli amusement park and the iconic “Little Mermaid” statue.',\n",
       " 'Odense is writer Hans Christian Andersen’s hometown, with a medieval core of cobbled streets and half-timbered houses']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing using NLTK\n",
    "import nltk\n",
    "\n",
    "data = \"Denmark is a Scandinavian country comprising the Jutland Peninsula and numerous islands. It's linked to nearby Sweden via the Öresund bridge. Copenhagen, its capital, is home to royal palaces and colorful Nyhavn harbor, plus the Tivoli amusement park and the iconic “Little Mermaid” statue. Odense is writer Hans Christian Andersen’s hometown, with a medieval core of cobbled streets and half-timbered houses\"\n",
    "\n",
    "nltk.sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daf19d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Denmark',\n",
       " 'is',\n",
       " 'a',\n",
       " 'Scandinavian',\n",
       " 'country',\n",
       " 'comprising',\n",
       " 'the',\n",
       " 'Jutland',\n",
       " 'Peninsula',\n",
       " 'and',\n",
       " 'numerous',\n",
       " 'islands',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'linked',\n",
       " 'to',\n",
       " 'nearby',\n",
       " 'Sweden',\n",
       " 'via',\n",
       " 'the',\n",
       " 'Öresund',\n",
       " 'bridge',\n",
       " '.',\n",
       " 'Copenhagen',\n",
       " ',',\n",
       " 'its',\n",
       " 'capital',\n",
       " ',',\n",
       " 'is',\n",
       " 'home',\n",
       " 'to',\n",
       " 'royal',\n",
       " 'palaces',\n",
       " 'and',\n",
       " 'colorful',\n",
       " 'Nyhavn',\n",
       " 'harbor',\n",
       " ',',\n",
       " 'plus',\n",
       " 'the',\n",
       " 'Tivoli',\n",
       " 'amusement',\n",
       " 'park',\n",
       " 'and',\n",
       " 'the',\n",
       " 'iconic',\n",
       " '“',\n",
       " 'Little',\n",
       " 'Mermaid',\n",
       " '”',\n",
       " 'statue',\n",
       " '.',\n",
       " 'Odense',\n",
       " 'is',\n",
       " 'writer',\n",
       " 'Hans',\n",
       " 'Christian',\n",
       " 'Andersen',\n",
       " '’',\n",
       " 's',\n",
       " 'hometown',\n",
       " ',',\n",
       " 'with',\n",
       " 'a',\n",
       " 'medieval',\n",
       " 'core',\n",
       " 'of',\n",
       " 'cobbled',\n",
       " 'streets',\n",
       " 'and',\n",
       " 'half-timbered',\n",
       " 'houses']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c0e977e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uztagger in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (0.0.7)\n",
      "Requirement already satisfied: UzMorphAnalyser in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from uztagger) (0.0.9)\n",
      "Requirement already satisfied: pandas in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from uztagger) (1.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from pandas->uztagger) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from pandas->uztagger) (1.22.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from pandas->uztagger) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->uztagger) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install uztagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd96c2c",
   "metadata": {},
   "source": [
    "# unpackaging & Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33995938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8005f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Apertag in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Apertag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620499e4",
   "metadata": {},
   "source": [
    "## POS Tags and Chunking\n",
    "\n",
    "There are 9 parts of speech in grammars, but in NLP there are more than 9 POS tags based on different set of rules, such as:\n",
    "\n",
    "* NN noun, singular 'table'\n",
    "* NNS noun plural 'tables'\n",
    "* NNP proper noun, singular \n",
    "* NNPS proper noun, plural \n",
    "\n",
    "There are 4 types of division for noun only. Similarly, there are multiple divisions for other part of speeches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e5e762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('see', 'VB'),\n",
       " ('an', 'DT'),\n",
       " ('example', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('POS', 'NNP'),\n",
       " ('tagging', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = 'We will see an example of POS tagging.'\n",
    "\n",
    "pos = nltk.pos_tag(nltk.word_tokenize(data))\n",
    "\n",
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143b5923",
   "metadata": {},
   "source": [
    "### Chunking \n",
    "\n",
    "After using parts of speech, Chunking can be used to make data more structured by giving a specific set of rules. Chunking is also known as shallow parser. \n",
    "Let's understand more about chunking by following example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cc7e640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  We/PRP\n",
      "  will/MD\n",
      "  see/VB\n",
      "  an/DT\n",
      "  (MN example/NN)\n",
      "  of/IN\n",
      "  (MN POS/NNP tagging/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "data =' We will see an example of POS tagging.'\n",
    "\n",
    "pos = nltk.pos_tag(nltk.word_tokenize(data))\n",
    "\n",
    "# now once the POS tag has been done. Let's say we want to further structure data such that Nouns are\n",
    "# categorized under one specific node defined by us :\n",
    "\n",
    "my_node = \"MN: {<NNP>*<NN>}\"\n",
    "\n",
    "chunk  =nltk.RegexpParser(my_node)\n",
    "result = chunk.parse(pos)\n",
    "print(result)\n",
    "result.draw()    # It will draw the pattern graphically which can be seen in Noun Phrase chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb861e78",
   "metadata": {},
   "source": [
    "### Graphical representation\n",
    "\n",
    "<img src=\"chunk.PNG\">\n",
    "\n",
    "\n",
    "We can see that both NN and NNP are now categorised into \"MN\" (as the given tag_name). \n",
    "\n",
    "So, whenever we need to categorise different tags into one tag, we can use chunking for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f769fe7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe3cd30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Denmark',\n",
       " 'Scandinavian',\n",
       " 'country',\n",
       " 'comprising',\n",
       " 'Jutland',\n",
       " 'Peninsula',\n",
       " 'numerous',\n",
       " 'islands',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'linked',\n",
       " 'nearby',\n",
       " 'Sweden',\n",
       " 'via',\n",
       " 'Öresund',\n",
       " 'bridge',\n",
       " 'Copenhagen',\n",
       " 'capital',\n",
       " 'home',\n",
       " 'royal',\n",
       " 'palaces',\n",
       " 'colorful',\n",
       " 'Nyhavn',\n",
       " 'harbor',\n",
       " 'plus',\n",
       " 'Tivoli',\n",
       " 'amusement',\n",
       " 'park',\n",
       " 'iconic',\n",
       " '“',\n",
       " 'Little',\n",
       " 'Mermaid',\n",
       " '”',\n",
       " 'statue',\n",
       " 'Odense',\n",
       " 'writer',\n",
       " 'Hans',\n",
       " 'Christian',\n",
       " 'Andersen',\n",
       " '’',\n",
       " 'hometown',\n",
       " 'medieval',\n",
       " 'core',\n",
       " 'cobbled',\n",
       " 'streets',\n",
       " 'half-timbered',\n",
       " 'houses']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "punct =string.punctuation\n",
    "\n",
    "data = \"Denmark is a Scandinavian country comprising the Jutland Peninsula and numerous islands. It's linked to nearby Sweden via the Öresund bridge. Copenhagen, its capital, is home to royal palaces and colorful Nyhavn harbor, plus the Tivoli amusement park and the iconic “Little Mermaid” statue. Odense is writer Hans Christian Andersen’s hometown, with a medieval core of cobbled streets and half-timbered houses\"\n",
    "clean_data =[]\n",
    "for word in nltk.word_tokenize(data):\n",
    "    if word not in punct:\n",
    "        if word not in stop_words:\n",
    "            clean_data.append(word)\n",
    "            \n",
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a69045f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter stemmer\n",
      "hobbi\n",
      "hobbi\n",
      "comput\n",
      "comput\n",
      "**************************\n",
      "lancaster stemmer\n",
      "hobby\n",
      "hobby\n",
      "comput\n",
      "comput\n",
      "**************************\n",
      "Snowball stemmer\n",
      "hobbi\n",
      "hobbi\n",
      "comput\n",
      "comput\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, SnowballStemmer\n",
    "\n",
    "lancaster = LancasterStemmer()\n",
    "porter = PorterStemmer()\n",
    "Snowball = SnowballStemmer(\"english\")\n",
    "print('Porter stemmer')\n",
    "print(porter.stem(\"hobby\"))\n",
    "print(porter.stem(\"hobbies\"))\n",
    "print(porter.stem(\"computer\"))\n",
    "print(porter.stem(\"computation\"))\n",
    "print(\"**************************\")  \n",
    "print('lancaster stemmer')\n",
    "print(lancaster.stem(\"hobby\"))\n",
    "print(lancaster.stem(\"hobbies\"))\n",
    "print(lancaster.stem(\"computer\"))\n",
    "print(porter.stem(\"computation\"))\n",
    "print(\"**************************\")  \n",
    "print('Snowball stemmer')\n",
    "print(Snowball.stem(\"hobby\"))\n",
    "print(Snowball.stem(\"hobbies\"))\n",
    "print(Snowball.stem(\"computer\"))\n",
    "print(Snowball.stem(\"computation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed557780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was go to the offic on my bike when i saw a car pass by hit the tree .\n",
      "i was going to the off on my bik when i saw a car pass by hit the tre .\n",
      "i wa go to the offic on my bike when i saw a car pass by hit the tree .\n"
     ]
    }
   ],
   "source": [
    "sent = \"I was going to the office on my bike when i saw a car passing by hit the tree.\"\n",
    "token = list(nltk.word_tokenize(sent))\n",
    "for stemmer in (Snowball, lancaster, porter):\n",
    "    stemm = [stemmer.stem(t) for t in token]\n",
    "    print(\" \".join(stemm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee116391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "print(porter.stem(\"running\"))\n",
    "print(porter.stem(\"runs\"))\n",
    "print(porter.stem(\"ran\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f64f985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "run\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "print(lemma.lemmatize('running'))\n",
    "print(lemma.lemmatize('runs'))\n",
    "print(lemma.lemmatize('ran'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b85a9888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "print(lemma.lemmatize('running',pos='v'))\n",
    "print(lemma.lemmatize('runs',pos='v'))\n",
    "print(lemma.lemmatize('ran',pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fe14cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Denmark/NNP)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  (GPE Nordic/JJ)\n",
      "  constituent/JJ\n",
      "  country/NN\n",
      "  in/IN\n",
      "  (GPE Northern/NNP Europe/NNP)\n",
      "  ./.\n",
      "  It/PRP\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  most/RBS\n",
      "  populous/JJ\n",
      "  and/CC\n",
      "  politically/RB\n",
      "  central/JJ\n",
      "  constituent/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Kingdom/NNP)\n",
      "  of/IN\n",
      "  (GPE Denmark/NNP))\n"
     ]
    }
   ],
   "source": [
    "sent = \"Denmark is a Nordic constituent country in Northern Europe. It is the most populous and politically central constituent of the Kingdom of Denmark\"\n",
    "\n",
    "words = nltk.word_tokenize(sent)\n",
    "pos_tag = nltk.pos_tag(words)\n",
    "namedEntity = nltk.ne_chunk(pos_tag)\n",
    "print(namedEntity)\n",
    "namedEntity.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbc46f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BllipParser',\n",
       " 'BottomUpChartParser',\n",
       " 'BottomUpLeftCornerChartParser',\n",
       " 'BottomUpProbabilisticChartParser',\n",
       " 'ChartParser',\n",
       " 'CoreNLPDependencyParser',\n",
       " 'CoreNLPParser',\n",
       " 'DependencyEvaluator',\n",
       " 'DependencyGraph',\n",
       " 'EarleyChartParser',\n",
       " 'FeatureBottomUpChartParser',\n",
       " 'FeatureBottomUpLeftCornerChartParser',\n",
       " 'FeatureChartParser',\n",
       " 'FeatureEarleyChartParser',\n",
       " 'FeatureIncrementalBottomUpChartParser',\n",
       " 'FeatureIncrementalBottomUpLeftCornerChartParser',\n",
       " 'FeatureIncrementalChartParser',\n",
       " 'FeatureIncrementalTopDownChartParser',\n",
       " 'FeatureTopDownChartParser',\n",
       " 'IncrementalBottomUpChartParser',\n",
       " 'IncrementalBottomUpLeftCornerChartParser',\n",
       " 'IncrementalChartParser',\n",
       " 'IncrementalLeftCornerChartParser',\n",
       " 'IncrementalTopDownChartParser',\n",
       " 'InsideChartParser',\n",
       " 'LeftCornerChartParser',\n",
       " 'LongestChartParser',\n",
       " 'MaltParser',\n",
       " 'NaiveBayesDependencyScorer',\n",
       " 'NonprojectiveDependencyParser',\n",
       " 'ParserI',\n",
       " 'ProbabilisticNonprojectiveParser',\n",
       " 'ProbabilisticProjectiveDependencyParser',\n",
       " 'ProjectiveDependencyParser',\n",
       " 'RandomChartParser',\n",
       " 'RecursiveDescentParser',\n",
       " 'ShiftReduceParser',\n",
       " 'SteppingChartParser',\n",
       " 'SteppingRecursiveDescentParser',\n",
       " 'SteppingShiftReduceParser',\n",
       " 'TestGrammar',\n",
       " 'TopDownChartParser',\n",
       " 'TransitionParser',\n",
       " 'UnsortedChartParser',\n",
       " 'ViterbiParser',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'api',\n",
       " 'bllip',\n",
       " 'chart',\n",
       " 'corenlp',\n",
       " 'dependencygraph',\n",
       " 'earleychart',\n",
       " 'evaluate',\n",
       " 'extract_test_sentences',\n",
       " 'featurechart',\n",
       " 'load_parser',\n",
       " 'malt',\n",
       " 'nonprojectivedependencyparser',\n",
       " 'pchart',\n",
       " 'projectivedependencyparser',\n",
       " 'recursivedescent',\n",
       " 'shiftreduce',\n",
       " 'transitionparser',\n",
       " 'util',\n",
       " 'viterbi']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nltk.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31225ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> \"saw\" | \"slept\" | \"walked\"\n",
    "  NP -> \"Rasmus\" | \"Anja\" | Det N | Det N PP\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76125539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Rasmus)\n",
      "  (VP (V saw) (NP Anja) (PP (P with) (NP (Det a) (N dog)))))\n"
     ]
    }
   ],
   "source": [
    "sent = \"Rasmus saw Anja with a dog\".split()\n",
    "parser = nltk.RecursiveDescentParser(grammar)\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree) \n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb28542c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram  : ['an', 'example', 'gram', 'is', 'of', 'this']\n",
      "2-gram  : ['an example', 'example of', 'is an', 'of gram', 'this is']\n",
      "3-gram  : ['an example of', 'example of gram', 'is an example', 'this is an']\n",
      "4-gram  : ['an example of gram', 'is an example of', 'this is an example']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# n-grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "string = [\"This is an example of n-gram!\"]\n",
    "vect1 = CountVectorizer(ngram_range=(1,1))\n",
    "vect1.fit_transform(string)\n",
    "vect2 = CountVectorizer(ngram_range=(2,2))\n",
    "vect2.fit_transform(string)\n",
    "vect3 = CountVectorizer(ngram_range=(3,3))\n",
    "vect3.fit_transform(string)\n",
    "vect4 = CountVectorizer(ngram_range=(4,4))\n",
    "vect4.fit_transform(string)\n",
    "print(\"1-gram  :\",vect1.get_feature_names())\n",
    "print(\"2-gram  :\",vect2.get_feature_names())\n",
    "print(\"3-gram  :\",vect3.get_feature_names())\n",
    "print(\"4-gram  :\",vect4.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05f8583a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words : ['an', 'bag', 'example', 'is', 'of', 'this', 'words']\n"
     ]
    }
   ],
   "source": [
    "## Bag Of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "string = [\"This is an example of bag of words!\"]\n",
    "vect1 = CountVectorizer()\n",
    "vect1.fit_transform(string)\n",
    "print(\"bag of words :\",vect1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5521a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>be</th>\n",
       "      <th>can</th>\n",
       "      <th>confusing</th>\n",
       "      <th>example</th>\n",
       "      <th>how</th>\n",
       "      <th>idf</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>see</th>\n",
       "      <th>this</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "      <th>works</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    an   be  can  confusing  example       how  idf   is        it       see  \\\n",
       "0  0.5  0.0  0.0        0.0      0.5  0.000000  0.0  0.5  0.000000  0.000000   \n",
       "1  0.0  0.0  0.0        0.0      0.0  0.408248  0.0  0.0  0.408248  0.408248   \n",
       "2  0.0  0.5  0.5        0.5      0.0  0.000000  0.5  0.0  0.000000  0.000000   \n",
       "\n",
       "   this        we      will     works  \n",
       "0   0.5  0.000000  0.000000  0.000000  \n",
       "1   0.0  0.408248  0.408248  0.408248  \n",
       "2   0.0  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfid = TfidfVectorizer(smooth_idf=False)\n",
    "\n",
    "doc= [\"This is an example.\",\"We will see how it works.\",\"IDF can be confusing\"]\n",
    "\n",
    "doc_vector = tfid.fit_transform(doc)\n",
    "#print(tfid.get_feature_names())\n",
    "df= pd.DataFrame(doc_vector.todense(),columns=tfid.get_feature_names())\n",
    "df\n",
    "#print(doc_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53070dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfid = TfidfVectorizer()\n",
    "\n",
    "doc= [\"Let's use python!\", \"Sklearn has package for Tf-idf.\",\"Vectorization is fun!\"]\n",
    "\n",
    "doc_vector = tfid.fit_transform(doc)\n",
    "#print(tfid.get_feature_names())\n",
    "df= pd.DataFrame(doc_vector.todense(),columns=tfid.get_feature_names())\n",
    "\n",
    "#print(doc_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51d8f17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>for</th>\n",
       "      <th>fun</th>\n",
       "      <th>has</th>\n",
       "      <th>idf</th>\n",
       "      <th>is</th>\n",
       "      <th>let</th>\n",
       "      <th>package</th>\n",
       "      <th>python</th>\n",
       "      <th>sklearn</th>\n",
       "      <th>tf</th>\n",
       "      <th>use</th>\n",
       "      <th>vectorization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        for      fun       has       idf       is      let   package   python  \\\n",
       "0  0.000000  0.00000  0.000000  0.000000  0.00000  0.57735  0.000000  0.57735   \n",
       "1  0.408248  0.00000  0.408248  0.408248  0.00000  0.00000  0.408248  0.00000   \n",
       "2  0.000000  0.57735  0.000000  0.000000  0.57735  0.00000  0.000000  0.00000   \n",
       "\n",
       "    sklearn        tf      use  vectorization  \n",
       "0  0.000000  0.000000  0.57735        0.00000  \n",
       "1  0.408248  0.408248  0.00000        0.00000  \n",
       "2  0.000000  0.000000  0.00000        0.57735  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dfb98a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbconvert in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (6.4.4)\n",
      "Requirement already satisfied: nbformat>=4.4 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from nbconvert) (5.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from nbconvert) (4.11.1)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from nbconvert) (2.14.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from nbconvert) (0.4)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from nbconvert) (0.5.13)\n",
      "Requirement already satisfied: traitlets>=5.0 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from nbconvert) (5.1.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from nbconvert) (1.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from nbconvert) (0.8.4)\n",
      "Requirement already satisfied: jinja2>=2.4 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from nbconvert) (3.0.3)\n",
      "Requirement already satisfied: testpath in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from nbconvert) (0.6.0)\n",
      "Requirement already satisfied: defusedxml in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from nbconvert) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from nbconvert) (0.1.2)\n",
      "Requirement already satisfied: jupyter-core in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from nbconvert) (4.10.0)\n",
      "Requirement already satisfied: bleach in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from nbconvert) (4.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from jinja2>=2.4->nbconvert) (2.1.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.5 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert) (7.2.2)\n",
      "Requirement already satisfied: nest-asyncio in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert) (1.5.5)\n",
      "Requirement already satisfied: pyzmq>=22.3 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (22.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (2.8.2)\n",
      "Requirement already satisfied: tornado>=6.0 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (6.1)\n",
      "Requirement already satisfied: fastjsonschema in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from nbformat>=4.4->nbconvert) (2.15.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from nbformat>=4.4->nbconvert) (4.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert) (0.18.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from beautifulsoup4->nbconvert) (2.3.1)\n",
      "Requirement already satisfied: webencodings in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from bleach->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: packaging in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from bleach->nbconvert) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from packaging->bleach->nbconvert) (3.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dd3f84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyppeteer in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: certifi>=2021 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from pyppeteer) (2022.6.15)\n",
      "Requirement already satisfied: pyee<9.0.0,>=8.1.0 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from pyppeteer) (8.2.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from pyppeteer) (4.64.1)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from pyppeteer) (10.4)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from pyppeteer) (1.26.12)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from pyppeteer) (1.4.4)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from pyppeteer) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from importlib-metadata>=1.4->pyppeteer) (3.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyppeteer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d29361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jinja2==3.0.3 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/shakeelsunny/miniconda3/envs/python3/lib/python3.9/site-packages (from jinja2==3.0.3) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jinja2==3.0.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
